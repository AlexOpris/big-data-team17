{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd0158b-222f-428c-9568-c1a6d8430014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .load(\"dbfs:/Volumes/methane/raw/emissions/\")\n",
    ")\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873279d2-3cde-45e7-9741-a70fd1fae791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# 1️⃣ Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Emissions Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2️⃣ Load your CSV file (replace path with your file)\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .load(\"dbfs:/Volumes/methane/raw/emissions/\")\n",
    ")\n",
    "\n",
    "df.display()\n",
    "\n",
    "# 3️⃣ Trim string columns\n",
    "string_cols = ['region', 'country', 'type', 'segment', 'reason']\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "# 4️⃣ Convert 'emissions' to double safely\n",
    "df = df.withColumn(\"emissions\", when(col(\"emissions\").rlike(\"^[0-9.]+$\"), col(\"emissions\").cast(DoubleType())).otherwise(None))\n",
    "\n",
    "# 5️⃣ Convert 'baseYear' to integer\n",
    "# - If a single year, keep it as int\n",
    "# - If interval like '2019-2021', take the average year and round\n",
    "from pyspark.sql.functions import split, floor\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"baseYear\",\n",
    "    when(col(\"baseYear\").rlike(\"^[0-9]{4}$\"), col(\"baseYear\").cast(IntegerType()))\n",
    "    .when(col(\"baseYear\").rlike(\"^[0-9]{4}-[0-9]{4}$\"),\n",
    "          floor((split(col(\"baseYear\"), \"-\").getItem(0).cast(IntegerType()) + \n",
    "                 split(col(\"baseYear\"), \"-\").getItem(1).cast(IntegerType())) / 2))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# 6️⃣ Drop unnecessary columns\n",
    "df = df.drop(\"_c0\", \"notes\")  # drop index and notes columns\n",
    "\n",
    "# 7️⃣ Show preprocessed DataFrame\n",
    "df.show(10, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CSVCleaningPipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
