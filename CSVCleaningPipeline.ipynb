{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45e1da6-b628-413f-aa05-3e0f4a7da1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .load(\"dbfs:/Volumes/methane/raw/emissions/\")\n",
    ")\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9890320-8298-4d79-99b0-5b39a978393c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# 1️⃣ Load CSV\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .load(\"dbfs:/Volumes/methane/raw/emissions/\")\n",
    ")\n",
    "\n",
    "# 2️⃣ List of year columns\n",
    "year_cols = [str(y) for y in range(1990, 2019)]\n",
    "\n",
    "# 3️⃣ Melt wide table to long format safely using try_cast\n",
    "exprs = [\n",
    "    F.struct(\n",
    "        F.lit(c).alias(\"Year\"),\n",
    "        F.expr(f\"try_cast(`{c}` as float)\").alias(\"Emission\")\n",
    "    )\n",
    "    for c in year_cols\n",
    "]\n",
    "\n",
    "df_long = df.withColumn(\"Year_Emission\", F.explode(F.array(*exprs)))\n",
    "df_long = df_long.withColumn(\"Year\", F.col(\"Year_Emission.Year\").cast(IntegerType())) \\\n",
    "                 .withColumn(\"Emission\", F.col(\"Year_Emission.Emission\")) \\\n",
    "                 .drop(\"Year_Emission\")\n",
    "\n",
    "# 4️⃣ Fill missing/null emissions\n",
    "df_long = df_long.fillna({\"Emission\": 0})\n",
    "\n",
    "# 5️⃣ Select only relevant columns\n",
    "df_long = df_long.select(\"Country\", \"Sector\", \"Year\", \"Emission\")\n",
    "\n",
    "# 6️⃣ Show final dataset\n",
    "df_long.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f74dc8-8e0e-4c74-a83a-c4202e474ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, array, posexplode, lag, avg\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3d1f92-4136-4aab-b6ed-6ec6b0d0e6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Filter for China and remove total sectors\n",
    "# ===============================\n",
    "df_china = df_long.filter(\n",
    "    (col(\"Country\") == \"China\") & \n",
    "    (~col(\"Sector\").isin([\"Total including LUCF\", \"Total excluding LUCF\"]))\n",
    ")\n",
    "display(df_china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c146c6b-e729-407e-b988-c898df42fc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 4: Create decomposition-style features per sector\n",
    "# ===============================\n",
    "# Window partitioned by sector to handle each sector independently\n",
    "w = Window.partitionBy(\"Sector\").orderBy(\"Year\")\n",
    "\n",
    "df_features = df_china.withColumn(\"lag_1\", lag(\"Emission\", 1).over(w)) \\\n",
    "                     .withColumn(\"lag_2\", lag(\"Emission\", 2).over(w)) \\\n",
    "                     .withColumn(\"lag_3\", lag(\"Emission\", 3).over(w)) \\\n",
    "                     .withColumn(\"roll_mean_3\", avg(\"Emission\").over(w.rowsBetween(-2, 0)))\n",
    "\n",
    "# Drop rows with nulls from lag features\n",
    "df_features = df_features.na.drop()\n",
    "display(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3412347b-c48a-44c4-bc8e-90380a3831e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 5: Prepare features for MLlib\n",
    "# ===============================\n",
    "feature_cols = [\"lag_1\", \"lag_2\", \"lag_3\", \"roll_mean_3\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "df_ml = assembler.transform(df_features)\n",
    "display(df_ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbbc101-4f6d-47e2-b95e-a61fe233ee91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 6: Split into train/test sets\n",
    "# ===============================\n",
    "train_df = df_ml.filter(col(\"year\") <= 2016)\n",
    "test_df  = df_ml.filter(col(\"year\") > 2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fcc6d30-ebcb-4d72-8dcf-4a9c45e08659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 7: Train Random Forest Regressor per sector\n",
    "# ===============================\n",
    "sectors = [row[\"Sector\"] for row in train_df.select(\"Sector\").distinct().collect()]\n",
    "models = {}\n",
    "predictions_list = []\n",
    "\n",
    "for sector in sectors:\n",
    "    df_train_sec = train_df.filter(col(\"Sector\") == sector)\n",
    "    df_test_sec  = test_df.filter(col(\"Sector\") == sector)\n",
    "\n",
    "    if df_train_sec.count() == 0 or df_test_sec.count() == 0:\n",
    "        continue\n",
    "\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Emission\", numTrees=100)\n",
    "    model = rf.fit(df_train_sec)\n",
    "    models[sector] = model\n",
    "\n",
    "    preds = model.transform(df_test_sec)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# Combine all predictions\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "if predictions_list:\n",
    "    predictions_all = reduce(DataFrame.unionByName, predictions_list)\n",
    "display(predictions_all.select(\"Sector\", \"Year\", \"Emission\", \"prediction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a8e8d4-443a-474b-b6be-eb1e1be4414a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 8: Evaluate RMSE per sector\n",
    "# ===============================\n",
    "evaluator = RegressionEvaluator(labelCol=\"Emission\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "for sector in sectors:\n",
    "    preds_sec = predictions_all.filter(col(\"Sector\") == sector)\n",
    "    rmse = evaluator.evaluate(preds_sec)\n",
    "    print(f\"Sector: {sector}, RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f0657d-3921-47b6-931d-e7ce48073214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 9: Forecast 5 years ahead per sector\n",
    "# ===============================\n",
    "n_years = 5\n",
    "\n",
    "# Get last year per sector\n",
    "last_known = df_features.groupBy(\"Sector\").agg({\"Year\": \"max\"}).withColumnRenamed(\"max(Year)\", \"Year\")\n",
    "last_data = df_features.join(last_known, on=[\"Sector\", \"Year\"], how=\"inner\")\n",
    "\n",
    "sector_dict = {}\n",
    "for row in last_data.collect():\n",
    "    sector = row[\"Sector\"]\n",
    "    sector_dict[sector] = {\n",
    "        \"lag_1\": row[\"lag_1\"],\n",
    "        \"lag_2\": row[\"lag_2\"],\n",
    "        \"lag_3\": row[\"lag_3\"],\n",
    "        \"roll_mean_3\": row[\"roll_mean_3\"],\n",
    "        \"last_year\": row[\"Year\"]\n",
    "    }\n",
    "\n",
    "forecast_rows = []\n",
    "\n",
    "for i in range(1, n_years + 1):\n",
    "    for sector, data in sector_dict.items():\n",
    "        # Create a Spark DataFrame for this row\n",
    "        future_df = spark.createDataFrame([Row(\n",
    "            lag_1=data[\"lag_1\"], lag_2=data[\"lag_2\"], lag_3=data[\"lag_3\"], roll_mean_3=data[\"roll_mean_3\"]\n",
    "        )])\n",
    "        future_vec = assembler.transform(future_df)\n",
    "        pred = models[sector].transform(future_vec).collect()[0][\"prediction\"]\n",
    "\n",
    "        # Save prediction\n",
    "        forecast_rows.append(Row(Sector=sector, year=data[\"last_year\"] + 1, Emission=None, prediction=pred))\n",
    "\n",
    "        # Update lags for next iteration\n",
    "        data[\"lag_3\"] = data[\"lag_2\"]\n",
    "        data[\"lag_2\"] = data[\"lag_1\"]\n",
    "        data[\"lag_1\"] = pred\n",
    "        data[\"roll_mean_3\"] = (data[\"lag_1\"] + data[\"lag_2\"] + data[\"lag_3\"]) / 3\n",
    "        data[\"last_year\"] += 1\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"Sector\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Emission\", DoubleType(), True),  # historical value, can be null\n",
    "    StructField(\"prediction\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame using explicit schema\n",
    "forecast_df = spark.createDataFrame(forecast_rows, schema=schema).orderBy(\"Sector\", \"Year\")\n",
    "display(forecast_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CSVCleaningPipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
